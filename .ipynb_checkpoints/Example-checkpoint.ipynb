{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring Synaptic Weights from Network Activity\n",
    "\n",
    "Here we compare plausible methods for overcoming the ''weight transport'' problem.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nasiryahm/STDWI/master/WeightTransportIllustration.png\" width=\"80%\">\n",
    "\n",
    "The illustration above depicts this issue. See the middle pane for the mathematical origin of the requirement of feedforward weight matrices for the error backpropagation.\n",
    "\n",
    "There are two key and recent methods which propose that the feedback error connectivity could be __learned__ by an inference process based upon the activity of the pre and post-synaptic neurons. These are regression discontinuity design (RDD) [1], and a rate-based method which we refer to as the Akrout method [2].\n",
    "\n",
    "We compare these methods against a simple and powerful method inspired by the analytic solution to a leaky-integrate and fire neuron.\n",
    "We refer to this method as Spike Timing-Dependent Weight Inference.\n",
    "\n",
    "[1] Guerguiev J, Kording KP, Richards BA. Spike-based causal inference for weight alignment. arXiv [q-bio.NC]. 2019. Available: http://arxiv.org/abs/1910.01689 \n",
    "\n",
    "[2] Akrout, M., Wilson, C., Humphreys, P. C., Lillicrap, T., & Tweed, D. (2019). Deep Learning without Weight Transport. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1904.05391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as spstats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from weight_inference import simulator\n",
    "from weight_inference import methods\n",
    "from weight_inference import fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot spike train without borders:\n",
    "def plot_spike_train(spike_trains, simulation_time, color='k', alpha='1.0', size=2500):\n",
    "    nb_units = len(spike_trains)\n",
    "    for i in range(nb_units):\n",
    "        spikes = spike_trains[i][spike_trains[i] < simulation_time]\n",
    "        plt.scatter(spikes, i*np.ones((len(spikes))),\n",
    "                    marker=\"|\", s=(size/nb_units), color=color, alpha=alpha)\n",
    "    \n",
    "    # Getting rid of bounding box\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the network shape, synaptic weights, and input stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Shape\n",
    "nb_input_neurons, nb_output_neurons = 100, 10\n",
    "\n",
    "# Simulation details\n",
    "simulation_time = 2e5 #ms\n",
    "# timestep = 0.25 #ms\n",
    "timestep = 2.5 #ms\n",
    "nb_timesteps = simulation_time // timestep\n",
    "\n",
    "# Stimulation Generation\n",
    "ratio_stimulated = 0.2\n",
    "stimulation_spike_trains = simulator.poisson_spike_train(\n",
    "        nb_input_neurons, 200/1000, simulation_time, timestep) # 200Hz stimulation\n",
    "stimulation_spike_trains = simulator.random_sample_spike_train(\n",
    "    stimulation_spike_trains, simulation_time, timestep, 100, ratio_stimulated) # resampling every 100ms with probability 0.1\n",
    "\n",
    "# Stimulation->Input weights\n",
    "# Creating weight matrices from stimulation to input and input to output\n",
    "stim_scale = 10.0\n",
    "stim_input_weights = stim_scale*np.eye(nb_input_neurons, nb_input_neurons) # One to one with a particular scale\n",
    "\n",
    "# Input->Output neuron synaptic weight matrix\n",
    "input_scale = 7.5*stim_scale # Compensating for hidden layer mean weight, firing rates and number)\n",
    "r = np.random.RandomState(seed=42)\n",
    "input_output_weights = input_scale*((np.sqrt(0.2)/np.sqrt(nb_input_neurons*ratio_stimulated))*r.normal(size=(nb_output_neurons, nb_input_neurons)) + (1 / (nb_input_neurons*ratio_stimulated)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the network activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing stimulation to input neurons\n",
    "stimulation_xpsps = simulator.spike_trains_to_xpsps(stimulation_spike_trains, simulation_time, timestep)\n",
    "\n",
    "# Computing input neuron activity\n",
    "input_neuron_acc, input_neuron_mem, input_neuron_spike_trains = simulator.lif_dynamics(\n",
    "        stimulation_xpsps, stim_input_weights, timestep)\n",
    "input_neuron_xpsps = simulator.spike_trains_to_xpsps(input_neuron_spike_trains, simulation_time, timestep)\n",
    "\n",
    "# Computing output neuron activity\n",
    "output_neuron_acc, output_neuron_mem, output_neuron_spike_trains = simulator.lif_dynamics(\n",
    "        input_neuron_xpsps, input_output_weights, timestep)\n",
    "output_neuron_xpsps = simulator.spike_trains_to_xpsps(output_neuron_spike_trains, simulation_time, timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising network activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6), dpi=100)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "ax = plot_spike_train(stimulation_spike_trains, 1e3, alpha=0.25, size=500)\n",
    "plt.xticks([0.0, 1e3], [0.0, 1.0])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Stimulation Neuron ID\")\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "ax = plot_spike_train(input_neuron_spike_trains, 1e3, alpha=0.25, size=500) #alpha=0.25, added here\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"Input Neuron ID\")\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "ax = plot_spike_train(output_neuron_spike_trains, 1e3,alpha=0.25, size=500, color='red') #alpha=0.25, added here\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"Output Neuron ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring synaptic weights from activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with a zero-centred small variance guess\n",
    "initial_guess_matrix = 0.001*(r.uniform(size=(nb_output_neurons, nb_input_neurons)) - 0.5)\n",
    "learning_rate = 5e-4\n",
    "check_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "akrout_guesses = fitter.akrout(\n",
    "            initial_guess_matrix,\n",
    "            input_neuron_spike_trains,\n",
    "            output_neuron_spike_trains,\n",
    "            simulation_time,\n",
    "            100,  # resample period\n",
    "            10,   # batch size\n",
    "            learning_rate,\n",
    "            check_interval)\n",
    "''' # commented out the section as it was causing an error, may be imp to look at\n",
    "tau_fast, tau_slow = 10.0, 200.0\n",
    "stdwi_guesses = fitter.stdwi(\n",
    "                initial_guess_matrix,\n",
    "                input_neuron_spike_trains,\n",
    "                output_neuron_spike_trains,\n",
    "                simulation_time,\n",
    "                100,\n",
    "                timestep,\n",
    "                tau_fast/tau_slow, tau_slow,\n",
    "                1.0, tau_fast,\n",
    "                learning_rate, check_interval)\n",
    "'''\n",
    "alpha = 0.05\n",
    "window_size = 40\n",
    "rdd_guesses = fitter.rdd(\n",
    "                initial_guess_matrix,\n",
    "                input_neuron_mem,\n",
    "                input_neuron_acc,\n",
    "                output_neuron_xpsps,\n",
    "                alpha, window_size,\n",
    "                1.0, # Spiking threshold\n",
    "                timestep,\n",
    "                learning_rate,\n",
    "                check_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "xmax = np.max(np.abs(input_output_weights.flatten()))*1.1\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(input_output_weights.flatten(), akrout_guesses[-1].flatten(), alpha=0.25)\n",
    "plt.xlim([-xmax, xmax])\n",
    "plt.ylim([-1.1*np.max(np.abs(akrout_guesses[-1].flatten())), 1.1*np.max(np.abs(akrout_guesses[-1].flatten()))])\n",
    "plt.title(\"Akrout Method\")\n",
    "''' # as before we are removing section dependent on stdwi_guesses\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(input_output_weights.flatten(), stdwi_guesses[-1].flatten(), alpha=0.25)\n",
    "plt.xlim([-xmax, xmax])\n",
    "plt.ylim([-1.1*np.max(np.abs(stdwi_guesses[-1].flatten())), 1.1*np.max(np.abs(stdwi_guesses[-1].flatten()))])\n",
    "plt.title(\"STDWI Method\")\n",
    "'''\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(input_output_weights.flatten(), rdd_guesses[-1].flatten(), alpha=0.25)\n",
    "plt.xlim([-xmax, xmax])\n",
    "plt.ylim([-1.1*np.max(np.abs(rdd_guesses[-1].flatten())), 1.1*np.max(np.abs(rdd_guesses[-1].flatten()))])\n",
    "plt.title(\"RDD Method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlations = [[], [], []]\n",
    "sign_accuracies = [[], [], []]\n",
    "\n",
    "results = [akrout_guesses, rdd_guesses]\n",
    "# older code that had stdwi guesses\n",
    "#results = [akrout_guesses, stdwi_guesses, rdd_guesses]\n",
    "\n",
    "for r_indx in range(len(results)):\n",
    "    for r_guess in results[r_indx]:\n",
    "        pearson_correlations[r_indx].append(spstats.pearsonr(r_guess.flatten(), input_output_weights.flatten())[0])\n",
    "        sign_accuracies[r_indx].append(methods.sign_alignment(r_guess.flatten(), input_output_weights.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Pearson Correlation Comparison')\n",
    "plt.plot(pearson_correlations[0], label='Akrout')\n",
    "plt.plot(pearson_correlations[1], label='STDWI')\n",
    "plt.plot(pearson_correlations[2], label='RDD')\n",
    "plt.xlabel(\"Simulation Time (s)\")\n",
    "plt.ylabel(\"Pearson Correlation Coefficient (r)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Sign Alignment Comparison')\n",
    "plt.plot(sign_accuracies[0], label='Akrout')\n",
    "plt.plot(sign_accuracies[1], label='STDWI')\n",
    "plt.plot(sign_accuracies[2], label='RDD')\n",
    "plt.xlabel(\"Simulation Time (s)\")\n",
    "plt.ylabel(\"Sign Alignment -- ratio\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_spike_trains(spike_trains, noise_level=0.05, simulation_time=None, timestep=None):\n",
    "    \"\"\"\n",
    "    Add random noise to spike trains by randomly adding or removing spikes.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_trains: list of spike times for each neuron\n",
    "    - noise_level: probability of adding/removing a spike\n",
    "    - simulation_time: total simulation time (for adding new spikes)\n",
    "    - timestep: simulation timestep (for adding new spikes)\n",
    "    \n",
    "    Returns:\n",
    "    - noisy_spike_trains: spike trains with added noise\n",
    "    \"\"\"\n",
    "    noisy_spike_trains = []\n",
    "    for spikes in spike_trains:\n",
    "        # Remove some spikes randomly\n",
    "        mask = np.random.random(len(spikes)) > noise_level\n",
    "        noisy_spikes = spikes[mask]\n",
    "        \n",
    "        # Add some random spikes\n",
    "        if simulation_time is not None and timestep is not None:\n",
    "            n_new_spikes = int(noise_level * len(spikes))\n",
    "            new_spikes = np.random.uniform(0, simulation_time, n_new_spikes)\n",
    "            noisy_spikes = np.sort(np.concatenate([noisy_spikes, new_spikes]))\n",
    "        \n",
    "        noisy_spike_trains.append(noisy_spikes)\n",
    "    \n",
    "    return noisy_spike_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the spike trains\n",
    "noisy_input_spike_trains = add_noise_to_spike_trains(input_neuron_spike_trains, \n",
    "                                                    noise_level=0.1,\n",
    "                                                    simulation_time=simulation_time,\n",
    "                                                    timestep=timestep)\n",
    "\n",
    "noisy_output_spike_trains = add_noise_to_spike_trains(output_neuron_spike_trains,\n",
    "                                                     noise_level=0.1,\n",
    "                                                     simulation_time=simulation_time,\n",
    "                                                     timestep=timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_akrout(initial_weights, input_spikes, output_spikes, \n",
    "                   simulation_time, batch_size, learning_rate, \n",
    "                   check_interval, max_weight=10.0, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Enhanced Akrout method with noise, weight constraints, and momentum.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_weights: initial weight matrix\n",
    "    - input_spikes: input neuron spike trains\n",
    "    - output_spikes: output neuron spike trains\n",
    "    - simulation_time: total simulation time\n",
    "    - batch_size: number of samples per batch\n",
    "    - learning_rate: initial learning rate\n",
    "    - check_interval: how often to check progress\n",
    "    - max_weight: maximum allowed weight value\n",
    "    - momentum: momentum parameter for smoother updates\n",
    "    \n",
    "    Returns:\n",
    "    - weight_history: list of weight matrices over time\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    weights = initial_weights.copy()\n",
    "    # weight_decay = 1e-4\n",
    "    # weights -= weight_decay * weights\n",
    "    weight_history = [weights.copy()]\n",
    "    velocity = np.zeros_like(weights)  # For momentum\n",
    "    \n",
    "    # Convert spike trains to continuous signals\n",
    "    input_xpsps = simulator.spike_trains_to_xpsps(input_spikes, simulation_time, timestep)\n",
    "    output_xpsps = simulator.spike_trains_to_xpsps(output_spikes, simulation_time, timestep)\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    n_batches = int(simulation_time // (batch_size * timestep))\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        # Calculate time window for this batch\n",
    "        start_time = batch * batch_size * timestep\n",
    "        end_time = (batch + 1) * batch_size * timestep\n",
    "        \n",
    "        # Get spikes in this batch\n",
    "        batch_input = input_xpsps[int(start_time/timestep):int(end_time/timestep)]\n",
    "        batch_output = output_xpsps[int(start_time/timestep):int(end_time/timestep)]\n",
    "        \n",
    "        # Calculate weight update (Akrout rule)\n",
    "        delta_weights = np.outer(batch_output.mean(axis=0), batch_input.mean(axis=0))\n",
    "\n",
    "        # max_grad = 1.0\n",
    "        # delta_weights = np.clip(delta_weights, -max_grad, max_grad)\n",
    "        \n",
    "        # Add momentum\n",
    "        velocity = momentum * velocity + (1 - momentum) * delta_weights\n",
    "        \n",
    "        # Update weights with adaptive learning rate\n",
    "        current_lr = learning_rate * (1 - batch/n_batches)  # Linear decay\n",
    "        weights += current_lr * velocity\n",
    "        \n",
    "        # Apply weight constraints\n",
    "        weights = np.clip(weights, 0, max_weight)  # Keep weights positive and bounded\n",
    "        \n",
    "        # Record weights at check intervals\n",
    "        if batch % check_interval == 0:\n",
    "            weight_history.append(weights.copy())\n",
    "    \n",
    "    return weight_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 10\n",
    "learning_rate = 5e-4\n",
    "check_interval = 10\n",
    "max_weight = 10.0\n",
    "momentum = 0.9\n",
    "\n",
    "# Run enhanced Akrout method\n",
    "enhanced_akrout_guesses = enhanced_akrout(\n",
    "    initial_guess_matrix,\n",
    "    noisy_input_spike_trains,  # Using noisy inputs\n",
    "    noisy_output_spike_trains, # Using noisy outputs\n",
    "    simulation_time,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    check_interval,\n",
    "    max_weight,\n",
    "    momentum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of weights\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot initial weights\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(initial_guess_matrix, cmap='viridis', aspect='auto')\n",
    "plt.title(\"Initial Weights\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot final weights (noiseless)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(akrout_guesses[-1], cmap='viridis', aspect='auto')\n",
    "plt.title(\"Final Weights (Noiseless)\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot final weights (noisy)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(enhanced_akrout_guesses[-1], cmap='viridis', aspect='auto')\n",
    "plt.title(\"Final Weights (Noisy)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debugging code right before calling enhanced_akrout()\n",
    "print(\"=== DEBUGGING SPIKE TRAIN DATA ===\")\n",
    "print(f\"Type of noisy_input_spike_trains: {type(noisy_input_spike_trains)}\")\n",
    "print(f\"Length of noisy_input_spike_trains: {len(noisy_input_spike_trains) if hasattr(noisy_input_spike_trains, '__len__') else 'No length'}\")\n",
    "\n",
    "if hasattr(noisy_input_spike_trains, '__len__') and len(noisy_input_spike_trains) > 0:\n",
    "    print(f\"Type of first spike train: {type(noisy_input_spike_trains[0])}\")\n",
    "    print(f\"First spike train content: {noisy_input_spike_trains[0]}\")\n",
    "    print(f\"Data type of spike times: {noisy_input_spike_trains[0].dtype if hasattr(noisy_input_spike_trains[0], 'dtype') else 'No dtype'}\")\n",
    "\n",
    "print(f\"simulation_time: {simulation_time}\")\n",
    "print(f\"timestep variable defined? {'timestep' in locals() or 'timestep' in globals()}\")\n",
    "\n",
    "# Check if timestep is defined, if not, what might it be?\n",
    "try:\n",
    "    print(f\"timestep value: {timestep}\")\n",
    "except NameError:\n",
    "    print(\"timestep is not defined!\")\n",
    "    print(\"Looking for similar variables...\")\n",
    "    # Check for common timestep variable names\n",
    "    for var_name in ['dt', 'time_step', 'delta_t', 'step_size']:\n",
    "        try:\n",
    "            print(f\"Found {var_name}: {eval(var_name)}\")\n",
    "        except NameError:\n",
    "            continue\n",
    "\n",
    "print(\"=== END DEBUGGING ===\")\n",
    "\n",
    "# Also add this inside your enhanced_akrout function, right before the spike_trains_to_xpsps calls:\n",
    "print(f\"Inside enhanced_akrout - timestep defined? {'timestep' in locals()}\")\n",
    "print(f\"Input spikes type: {type(input_spikes)}\")\n",
    "print(f\"Input spikes shape/length: {len(input_spikes) if hasattr(input_spikes, '__len__') else 'No length'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
